{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMogELrSLf2qPHVwsdNO1VQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kennyluke1023/kennyluke1023/blob/main/Pyspark_Create.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBIKQD3A9ezw"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create RDD from .txt"
      ],
      "metadata": {
        "id": "Kp__Fqlp97Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.context import SparkContext, SparkConf\n",
        "\n",
        "#from pyspark import SparkConf, SparkContext\n",
        "#import collections\n",
        "\n",
        "#conf = SparkConf().setMaster(\"local\").setAppName(\"RatingsHistogram\")\n",
        "#sc = SparkContext(conf = conf)\n",
        "\n",
        "\n",
        "''' build up an enviremnt\n",
        "create sparkconf()first and create a context().\n",
        "Remind if your local has been created a Spark context, use getOrcreate() to create a new one\n",
        "\n",
        "'''\n",
        "conf = SparkConf().setMaster('local').setAppName('Textfile')\n",
        "sc = SparkContext.getOrCreate(conf=conf )"
      ],
      "metadata": {
        "id": "81U7ISNt96yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''' read textfiles for the local.\n",
        "textFile(): read all the txt files and save as one RDD\n",
        "wholetextFile() read all the text files but one by one\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "id": "6qkbuXFBDe2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word counts Function"
      ],
      "metadata": {
        "id": "Gcs2o4GpJA1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### textFiles()"
      ],
      "metadata": {
        "id": "kAduZDXHO-aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kD1D-DwWPFi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = sc.textFile('/content/data')\n",
        "# functiom\n",
        "sp_= lambda x : x.split(\" \")\n",
        "st_= lambda x: x.strip()\n",
        "map_= lambda x :(x,1)\n",
        "count_=lambda x, y: x+y\n",
        "\n",
        "# work flow\n",
        "pylines = lines.flatMap(sp_).filter(st_).map(map_).reduceByKey(count_).sortBy(lambda x: x[1],False) ## sort by defult is True that is ascending.\n",
        "output =pylines.collect()\n",
        "\n",
        "for word, count in output:\n",
        "  print(word, count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU_oxA0FUq8g",
        "outputId": "ae3b55ec-3ea3-421b-a7b7-4fa01ea4c8b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello 5\n",
            "Spark 2\n",
            "python 1\n",
            "Python 1\n",
            "World 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### wholetextFile()\n",
        "分開文件，獨立進行\n",
        "\n",
        "issues\n",
        "遇上問題，如何把他們都合並??\n",
        "把\\R\\N\\等等的文字都去掉??\n",
        "\n",
        "之後則完美了"
      ],
      "metadata": {
        "id": "Pvd30H9NPBuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines_2 = sc.wholeTextFiles('/content/data')\n",
        "lines_2.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT38hm9-JAeg",
        "outputId": "a85b16f9-83c8-44b4-d8f3-f6003be8f908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('file:/content/data/1.txt', 'Hello python \\r\\nHello Spark'),\n",
              " ('file:/content/data/2.txt', 'Hello World\\r\\nHello Spark\\r\\nHello Python')]"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP5s6b8leEi8",
        "outputId": "f1e3cd06-fc03-42f0-ea69-00a20825929b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hello', 2),\n",
              " ('python', 1),\n",
              " ('Spark', 1),\n",
              " ('Python', 1),\n",
              " ('\\r\\nHello', 1),\n",
              " ('World\\r\\nHello', 1),\n",
              " ('Spark\\r\\nHello', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines= sc.parallelize(lst)\n",
        "pylines = lines.flatMap(sp_).filter(st_).map(map_).reduceByKey(count_).sortBy(lambda x: x[1],False) ## sort by defult is True that is ascending.\n",
        "output =pylines.collect()\n",
        "for word, count in output:\n",
        "  print(word, count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwuUeeidKK5V",
        "outputId": "b3584391-83ab-4af5-a23d-a6efe7346853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello 2\n",
            "python 1\n",
            "Spark 1\n",
            "Python 1\n",
            "\r\n",
            "Hello 1\n",
            "World\r\n",
            "Hello 1\n",
            "Spark\r\n",
            "Hello 1\n"
          ]
        }
      ]
    }
  ]
}